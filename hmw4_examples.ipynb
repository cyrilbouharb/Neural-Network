{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1R9rUIyQ-Rws"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S57npX2L-WtZ"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, sizes, initial_weights=None, eta=0.01, reg_lambda=0.01, use_softmax=False):\n",
    "        self.sizes = sizes\n",
    "        self.eta = eta\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.use_softmax = use_softmax\n",
    "        self.weights = self.init_weights() if initial_weights is None else initial_weights\n",
    "\n",
    "    def init_weights(self):\n",
    "        return [np.random.randn(y, x + 1) * 0.1 for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def add_bias(self, X):\n",
    "        return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        act = [self.add_bias(X)]\n",
    "        ll = []\n",
    "        for index, weight in enumerate(self.weights[:-1]):\n",
    "            ll.append(act[-1] @ weight.T)\n",
    "            act.append(self.add_bias(1 / (1 + np.exp(-ll[-1]))))\n",
    "        ll.append(act[-1] @ self.weights[-1].T)\n",
    "        act.append(self.softmax(ll[-1]) if self.use_softmax else 1 / (1 + np.exp(-ll[-1])))\n",
    "        return act, ll\n",
    "\n",
    "    def cost(self, Y, act):\n",
    "        xq = Y.shape[0]\n",
    "        return -np.sum(Y * np.log(act[-1] + 1e-8)) / xq if self.use_softmax else -np.sum(Y * np.log(act[-1]) + (1 - Y) * np.log(1 - act[-1])) / xq\n",
    "\n",
    "    def backward(self, targets, activations, weights):\n",
    "        num_samples = targets.shape[0]\n",
    "        errors = [activations[-1] - targets]\n",
    "        gradients = []\n",
    "        for i in range(len(weights) - 1, 0, -1):\n",
    "            delta = (errors[0] @ weights[i][:, 1:]) * activations[i][:, 1:] * (1 - activations[i][:, 1:])\n",
    "            errors.insert(0, delta)\n",
    "        for i in range(len(weights)):\n",
    "            grad = errors[i].T @ activations[i] / num_samples\n",
    "            gradients.append(grad)\n",
    "        return errors, gradients\n",
    "\n",
    "    def update_weights(self, gradients):\n",
    "        for i in range(len(self.weights)):\n",
    "            regularization_grad = (self.reg_lambda * self.weights[i]) / self.weights[i].shape[0]\n",
    "            regularization_grad[:, 0] = 0\n",
    "            self.weights[i] -= self.eta * (gradients[i] + regularization_grad)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        final_activations, _ = self.forward(inputs)\n",
    "        return np.argmax(final_activations[-1], axis=1) + 1\n",
    "\n",
    "    def train(self, inputs, labels, iterations):\n",
    "        for _ in range(iterations):\n",
    "            final_activations, _ = self.forward(inputs)\n",
    "            _, weight_gradients = self.backward(labels, final_activations, self.weights)\n",
    "            self.update_weights(weight_gradients)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x5dapdeb-uOn"
   },
   "outputs": [],
   "source": [
    "def examples_testing(nn, X_example, Y_example):\n",
    "    print(f\"Regularization parameter lambda={nn.reg_lambda:.3f}\")\n",
    "    print(\"\\nInitializing the network with the following structure (number of neurons per layer):\")\n",
    "    neuron_counts = [nn.weights[0].shape[1] - 1] + [layer.shape[0] for layer in nn.weights]\n",
    "    print(neuron_counts)\n",
    "    print(\"\\nInitial weights:\")\n",
    "    for idx, layer in enumerate(nn.weights):\n",
    "        print(f\"\\nInitial Theta{idx+1} (the weights of each neuron, including the bias weight, are stored in the rows):\")\n",
    "        for row in layer:\n",
    "            print('\\t' + '  '.join(f\"{weight:.5f}\" for weight in row))\n",
    "    print(\"\\nTraining set\")\n",
    "    total_cost = 0\n",
    "    forward_results = []\n",
    "    for i in range(len(X_example)):\n",
    "        print(f\"\\n\\tTraining instance {i+1}\")\n",
    "        x_str = '  '.join(f\"{num:.5f}\" for num in X_example[i])\n",
    "        print(f\"\\t\\tx: [{x_str}]\")\n",
    "        y_str = '  '.join(f\"{num:.5f}\" for num in Y_example[i])\n",
    "        print(f\"\\t\\ty: [{y_str}]\")\n",
    "    print('--------------------------------------------')\n",
    "    print(\"Computing the error/cost, J, of the network\")\n",
    "    for i in range(len(X_example)):\n",
    "        print(f\"Processing training instance {i+1}\")\n",
    "        activations, zs = nn.forward(np.array([X_example[i]]))\n",
    "        cost = nn.cost(np.array([Y_example[i]]), activations[-1])\n",
    "        total_cost += cost\n",
    "        forward_results.append((activations, zs, cost))\n",
    "        print(f\"\\t\\tForward propagating the input [{x_str}]\")\n",
    "        for j, a in enumerate(activations):\n",
    "            a_str = '  '.join(f\"{x:.5f}\" for x in a[0])\n",
    "            print(f\"\\t\\ta{j+1}: [{a_str}]\")\n",
    "            if j < len(zs):\n",
    "                z_str = '  '.join(f\"{x:.5f}\" for x in zs[j][0])\n",
    "                print(f\"\\t\\tz{j+2}: [{z_str}]\")\n",
    "        pred_output = activations[-1][0]\n",
    "        pred_output_str = '  '.join(f\"{p:.5f}\" for p in pred_output)\n",
    "        y_str = '  '.join(f\"{num:.5f}\" for num in Y_example[i])\n",
    "        print(f\"\\t\\tf(x): [{pred_output_str}]\")\n",
    "        print(f\"\\t\\tPredicted output for instance {i+1}: [{pred_output_str}]\")\n",
    "        print(f\"\\t\\tExpected output for instance {i+1}: [{y_str}]\")\n",
    "        print(f\"\\t\\tCost, J, associated with instance {i+1}: {cost:.3f}\")\n",
    "    average_unregularized_cost = total_cost / len(X_example)\n",
    "    reg_cost = (nn.reg_lambda / (2 * len(X_example))) * sum(np.sum(np.square(w[:, 1:])) for w in nn.weights)\n",
    "    final_cost = average_unregularized_cost + reg_cost\n",
    "    print(f\"\\nFinal (regularized) cost, J, based on the complete training set: {final_cost:.5f}\\n\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(\"Running backpropagation\")\n",
    "    all_gradients = [[] for _ in nn.weights]\n",
    "    for i, (activations, zs, cost) in enumerate(forward_results):\n",
    "        print(f\"\\n\\tComputing gradients based on training instance {i+1}\")\n",
    "        deltas, gradients = nn.backward(np.array([Y_example[i]]), activations, nn.weights)\n",
    "        for j in reversed(range(len(deltas))):\n",
    "            delta_str = '  '.join(f\"{x:.5f}\" for x in deltas[j].flatten())\n",
    "            print(f\"\\t\\tdelta{j+2}: [{delta_str}]\")\n",
    "        for idx in reversed(range(len(gradients))):\n",
    "            all_gradients[idx].append(gradients[idx])\n",
    "            grad_str = '\\n\\t\\t'.join('  '.join(f\"{g:.5f}\" for g in row) for row in gradients[idx])\n",
    "            print(f\"\\n\\t\\tGradients of Theta{idx+1} based on training instance {i+1}:\")\n",
    "            print(f\"\\t\\t{grad_str}\")\n",
    "    print(\"\\nThe entire training set has been processed. Computing the average (regularized) gradients:\")\n",
    "    for idx in range(len(all_gradients)):\n",
    "        avg_grad = np.mean(all_gradients[idx], axis=0)\n",
    "        avg_grad[:, 1:] += (nn.reg_lambda / len(X_example)) * nn.weights[idx][:, 1:]\n",
    "        avg_grad_str = '\\n\\t'.join('  '.join(f\"{g:.5f}\" for g in row) for row in avg_grad)\n",
    "        print(f\"\\n\\tFinal regularized gradients of Theta{idx + 1}:\")\n",
    "        print(f\"\\t{avg_grad_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsW3YTQ4-0KI",
    "outputId": "b6ed61ca-0314-4527-9f70-212aba338b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter lambda=0.000\n",
      "\n",
      "Initializing the network with the following structure (number of neurons per layer):\n",
      "[1, 2, 1]\n",
      "\n",
      "Initial weights:\n",
      "\n",
      "Initial Theta1 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
      "\t0.40000  0.10000\n",
      "\t0.30000  0.20000\n",
      "\n",
      "Initial Theta2 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
      "\t0.70000  0.50000  0.60000\n",
      "\n",
      "Training set\n",
      "\n",
      "\tTraining instance 1\n",
      "\t\tx: [0.13000]\n",
      "\t\ty: [0.90000]\n",
      "\n",
      "\tTraining instance 2\n",
      "\t\tx: [0.42000]\n",
      "\t\ty: [0.23000]\n",
      "--------------------------------------------\n",
      "Computing the error/cost, J, of the network\n",
      "Processing training instance 1\n",
      "\t\tForward propagating the input [0.42000]\n",
      "\t\ta1: [1.00000  0.13000]\n",
      "\t\tz2: [0.41300  0.32600]\n",
      "\t\ta2: [1.00000  0.60181  0.58079]\n",
      "\t\tz3: [1.34937]\n",
      "\t\ta3: [0.79403]\n",
      "\t\tf(x): [0.79403]\n",
      "\t\tPredicted output for instance 1: [0.79403]\n",
      "\t\tExpected output for instance 1: [0.90000]\n",
      "\t\tCost, J, associated with instance 1: 0.366\n",
      "Processing training instance 2\n",
      "\t\tForward propagating the input [0.42000]\n",
      "\t\ta1: [1.00000  0.42000]\n",
      "\t\tz2: [0.44200  0.38400]\n",
      "\t\ta2: [1.00000  0.60874  0.59484]\n",
      "\t\tz3: [1.36127]\n",
      "\t\ta3: [0.79597]\n",
      "\t\tf(x): [0.79597]\n",
      "\t\tPredicted output for instance 2: [0.79597]\n",
      "\t\tExpected output for instance 2: [0.23000]\n",
      "\t\tCost, J, associated with instance 2: 1.276\n",
      "\n",
      "Final (regularized) cost, J, based on the complete training set: 0.82098\n",
      "\n",
      "--------------------------------------------\n",
      "Running backpropagation\n",
      "\n",
      "\tComputing gradients based on training instance 1\n",
      "\t\tdelta3: [-0.10597]\n",
      "\t\tdelta2: [-0.01270  -0.01548]\n",
      "\n",
      "\t\tGradients of Theta2 based on training instance 1:\n",
      "\t\t-0.10597  -0.06378  -0.06155\n",
      "\n",
      "\t\tGradients of Theta1 based on training instance 1:\n",
      "\t\t-0.01270  -0.00165\n",
      "\t\t-0.01548  -0.00201\n",
      "\n",
      "\tComputing gradients based on training instance 2\n",
      "\t\tdelta3: [0.56597]\n",
      "\t\tdelta2: [0.06740  0.08184]\n",
      "\n",
      "\t\tGradients of Theta2 based on training instance 2:\n",
      "\t\t0.56597  0.34452  0.33666\n",
      "\n",
      "\t\tGradients of Theta1 based on training instance 2:\n",
      "\t\t0.06740  0.02831\n",
      "\t\t0.08184  0.03437\n",
      "\n",
      "The entire training set has been processed. Computing the average (regularized) gradients:\n",
      "\n",
      "\tFinal regularized gradients of Theta1:\n",
      "\t0.02735  0.01333\n",
      "\t0.03318  0.01618\n",
      "\n",
      "\tFinal regularized gradients of Theta2:\n",
      "\t0.23000  0.14037  0.13756\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [2, 2, 1]\n",
    "weights_example = [np.array([[0.4, 0.1], [0.3, 0.2]]), np.array([[0.7, 0.5, 0.6]])]\n",
    "nn = NN(layer_sizes,weights_example, eta=0.01, reg_lambda=0.00, use_softmax=False)\n",
    "examples_testing(nn, np.array([[0.13], [0.42]]), np.array([[0.9], [0.23]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhhanHBC_p7Q"
   },
   "source": [
    "**Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MQ3lsuu_r3W",
    "outputId": "3c3a8a40-1621-4b82-f853-a49574c53003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter lambda=0.250\n",
      "\n",
      "Initializing the network with the following structure (number of neurons per layer):\n",
      "[2, 4, 3, 2]\n",
      "\n",
      "Initial weights:\n",
      "\n",
      "Initial Theta1 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
      "\t0.42000  0.15000  0.40000\n",
      "\t0.72000  0.10000  0.54000\n",
      "\t0.01000  0.19000  0.42000\n",
      "\t0.30000  0.35000  0.68000\n",
      "\n",
      "Initial Theta2 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
      "\t0.21000  0.67000  0.14000  0.96000  0.87000\n",
      "\t0.87000  0.42000  0.20000  0.32000  0.89000\n",
      "\t0.03000  0.56000  0.80000  0.69000  0.09000\n",
      "\n",
      "Initial Theta3 (the weights of each neuron, including the bias weight, are stored in the rows):\n",
      "\t0.04000  0.87000  0.42000  0.53000\n",
      "\t0.17000  0.10000  0.95000  0.69000\n",
      "\n",
      "Training set\n",
      "\n",
      "\tTraining instance 1\n",
      "\t\tx: [0.32000  0.68000]\n",
      "\t\ty: [0.75000  0.98000]\n",
      "\n",
      "\tTraining instance 2\n",
      "\t\tx: [0.83000  0.02000]\n",
      "\t\ty: [0.75000  0.28000]\n",
      "--------------------------------------------\n",
      "Computing the error/cost, J, of the network\n",
      "Processing training instance 1\n",
      "\t\tForward propagating the input [0.83000  0.02000]\n",
      "\t\ta1: [1.00000  0.32000  0.68000]\n",
      "\t\tz2: [0.74000  1.11920  0.35640  0.87440]\n",
      "\t\ta2: [1.00000  0.67700  0.75384  0.58817  0.70566]\n",
      "\t\tz3: [1.94769  2.12136  1.48154]\n",
      "\t\ta3: [1.00000  0.87519  0.89296  0.81480]\n",
      "\t\tz4: [1.60831  1.66805]\n",
      "\t\ta4: [0.83318  0.84132]\n",
      "\t\tf(x): [0.83318  0.84132]\n",
      "\t\tPredicted output for instance 1: [0.83318  0.84132]\n",
      "\t\tExpected output for instance 1: [0.75000  0.98000]\n",
      "\t\tCost, J, associated with instance 1: 0.791\n",
      "Processing training instance 2\n",
      "\t\tForward propagating the input [0.83000  0.02000]\n",
      "\t\ta1: [1.00000  0.83000  0.02000]\n",
      "\t\tz2: [0.55250  0.81380  0.17610  0.60410]\n",
      "\t\ta2: [1.00000  0.63472  0.69292  0.54391  0.64659]\n",
      "\t\tz3: [1.81696  2.02468  1.37327]\n",
      "\t\ta3: [1.00000  0.86020  0.88336  0.79791]\n",
      "\t\tz4: [1.58228  1.64577]\n",
      "\t\ta4: [0.82953  0.83832]\n",
      "\t\tf(x): [0.82953  0.83832]\n",
      "\t\tPredicted output for instance 2: [0.82953  0.83832]\n",
      "\t\tExpected output for instance 2: [0.75000  0.28000]\n",
      "\t\tCost, J, associated with instance 2: 1.944\n",
      "\n",
      "Final (regularized) cost, J, based on the complete training set: 1.90351\n",
      "\n",
      "--------------------------------------------\n",
      "Running backpropagation\n",
      "\n",
      "\tComputing gradients based on training instance 1\n",
      "\t\tdelta4: [0.08318  -0.13868]\n",
      "\t\tdelta3: [0.00639  -0.00925  -0.00779]\n",
      "\t\tdelta2: [-0.00087  -0.00133  -0.00053  -0.00070]\n",
      "\n",
      "\t\tGradients of Theta3 based on training instance 1:\n",
      "\t\t0.08318  0.07280  0.07427  0.06777\n",
      "\t\t-0.13868  -0.12138  -0.12384  -0.11300\n",
      "\n",
      "\t\tGradients of Theta2 based on training instance 1:\n",
      "\t\t0.00639  0.00433  0.00482  0.00376  0.00451\n",
      "\t\t-0.00925  -0.00626  -0.00698  -0.00544  -0.00653\n",
      "\t\t-0.00779  -0.00527  -0.00587  -0.00458  -0.00550\n",
      "\n",
      "\t\tGradients of Theta1 based on training instance 1:\n",
      "\t\t-0.00087  -0.00028  -0.00059\n",
      "\t\t-0.00133  -0.00043  -0.00091\n",
      "\t\t-0.00053  -0.00017  -0.00036\n",
      "\t\t-0.00070  -0.00022  -0.00048\n",
      "\n",
      "\tComputing gradients based on training instance 2\n",
      "\t\tdelta4: [0.07953  0.55832]\n",
      "\t\tdelta3: [0.01503  0.05809  0.06892]\n",
      "\t\tdelta2: [0.01694  0.01465  0.01999  0.01622]\n",
      "\n",
      "\t\tGradients of Theta3 based on training instance 2:\n",
      "\t\t0.07953  0.06841  0.07025  0.06346\n",
      "\t\t0.55832  0.48027  0.49320  0.44549\n",
      "\n",
      "\t\tGradients of Theta2 based on training instance 2:\n",
      "\t\t0.01503  0.00954  0.01042  0.00818  0.00972\n",
      "\t\t0.05809  0.03687  0.04025  0.03160  0.03756\n",
      "\t\t0.06892  0.04374  0.04775  0.03748  0.04456\n",
      "\n",
      "\t\tGradients of Theta1 based on training instance 2:\n",
      "\t\t0.01694  0.01406  0.00034\n",
      "\t\t0.01465  0.01216  0.00029\n",
      "\t\t0.01999  0.01659  0.00040\n",
      "\t\t0.01622  0.01346  0.00032\n",
      "\n",
      "The entire training set has been processed. Computing the average (regularized) gradients:\n",
      "\n",
      "\tFinal regularized gradients of Theta1:\n",
      "\t0.00804  0.02564  0.04987\n",
      "\t0.00666  0.01837  0.06719\n",
      "\t0.00973  0.03196  0.05252\n",
      "\t0.00776  0.05037  0.08492\n",
      "\n",
      "\tFinal regularized gradients of Theta2:\n",
      "\t0.01071  0.09068  0.02512  0.12597  0.11586\n",
      "\t0.02442  0.06780  0.04164  0.05308  0.12677\n",
      "\t0.03056  0.08924  0.12094  0.10270  0.03078\n",
      "\n",
      "\tFinal regularized gradients of Theta3:\n",
      "\t0.08135  0.17935  0.12476  0.13186\n",
      "\t0.20982  0.19195  0.30343  0.25249\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [2, 4, 3, 2]\n",
    "weights_example =  [np.array([[0.42, 0.15, 0.4], [0.72, 0.1, 0.54], [0.01, 0.19, 0.42], [0.3, 0.35, 0.68]]),\n",
    "    np.array([[0.21, 0.67, 0.14, 0.96, 0.87], [0.87, 0.42, 0.2, 0.32, 0.89], [0.03, 0.56, 0.8, 0.69, 0.09]]),\n",
    "    np.array([[0.04, 0.87, 0.42, 0.53], [0.17, 0.1, 0.95, 0.69]])]\n",
    "nn = NN(layer_sizes, weights_example, eta=0.01, reg_lambda=0.25, use_softmax=False)\n",
    "examples_testing(nn, np.array([[0.32, 0.68], [0.83, 0.02]]), np.array([[0.75, 0.98], [0.75, 0.28]]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
